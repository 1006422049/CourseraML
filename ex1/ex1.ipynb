{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Linear regression with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex1data1.txt'\n",
    "cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1),unpack=True) #Read in comma separated data\n",
    "#Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size # number of training examples\n",
    "#Insert the usual column of 1's into the \"X\" matrix\n",
    "X = np.insert(X,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Plotting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the data to see what it looks like\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X[:,1],y[:,0],'rx',markersize=10)\n",
    "plt.grid(True) #Always plot.grid true!\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def h(theta,X): #Linear hypothesis function\n",
    "    return np.dot(X,theta)\n",
    "\n",
    "def computeCost(mytheta,X,y): #Cost function\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    y is a matrix with m- rows and 1 column\n",
    "    \"\"\"\n",
    "    #note to self: *.shape is (rows, columns)\n",
    "    return float((1./(2*m)) * np.dot((h(mytheta,X)-y).T,(h(mytheta,X)-y)))\n",
    "\n",
    "#Test that running computeCost with 0's as theta returns 32.07:\n",
    "\n",
    "initial_theta = np.zeros((X.shape[1],1)) #(theta is a vector with n rows and 1 columns (if X has n features) )\n",
    "print computeCost(initial_theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Actual gradient descent minimizing routine\n",
    "def descendGradient(X, theta_start = np.zeros(2)):\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    \"\"\"\n",
    "    theta = theta_start\n",
    "    jvec = [] #Used to plot cost as function of iteration\n",
    "    thetahistory = [] #Used to visualize the minimization path later on\n",
    "    for meaninglessvariable in xrange(iterations):\n",
    "        tmptheta = theta\n",
    "        jvec.append(computeCost(theta,X,y))\n",
    "        thetahistory.append(list(tmptheta))\n",
    "        #Simultaneously updating theta values\n",
    "        for j in xrange(len(tmptheta)):\n",
    "            tmptheta[j] = theta[j] - (alpha/m)*np.sum((h(initial_theta,X) - y)*np.array(X[:,j]).reshape(m,1))\n",
    "        theta = tmptheta\n",
    "    return theta, thetahistory, jvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Actually run gradient descent to get the best-fit theta values\n",
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "theta, thetahistory, jvec = descendGradient(X,initial_theta)\n",
    "\n",
    "#Plot the convergence of the cost function\n",
    "def plotConvergence(jvec):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(len(jvec)),jvec,'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
    "    #dummy = plt.ylim([4,8])\n",
    "\n",
    "\n",
    "plotConvergence(jvec)\n",
    "dummy = plt.ylim([4,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the line on top of the data to ensure it looks correct\n",
    "def myfit(xval):\n",
    "    return theta[0] + theta[1]*xval\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X[:,1],y[:,0],'rx',markersize=10,label='Training Data')\n",
    "plt.plot(X[:,1],myfit(X[:,1]),'b-',label = 'Hypothesis: h(x) = %0.2f + %0.2fx'%(theta[0],theta[1]))\n",
    "plt.grid(True) #Always plot.grid true!\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Visualizing _J($\\theta$)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import necessary matplotlib tools for 3d plots\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from matplotlib import cm\n",
    "import itertools\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "xvals = np.arange(-10,10,.5)\n",
    "yvals = np.arange(-1,4,.1)\n",
    "myxs, myys, myzs = [], [], []\n",
    "for david in xvals:\n",
    "    for kaleko in yvals:\n",
    "        myxs.append(david)\n",
    "        myys.append(kaleko)\n",
    "        myzs.append(computeCost(np.array([[david], [kaleko]]),X,y))\n",
    "\n",
    "scat = ax.scatter(myxs,myys,myzs,c=np.abs(myzs),cmap=plt.get_cmap('YlOrRd'))\n",
    "\n",
    "plt.xlabel(r'$\\theta_0$',fontsize=30)\n",
    "plt.ylabel(r'$\\theta_1$',fontsize=30)\n",
    "plt.title('Cost (Minimization Path Shown in Blue)',fontsize=30)\n",
    "plt.plot([x[0] for x in thetahistory],[x[1] for x in thetahistory],jvec,'bo-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear Regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex1data2.txt'\n",
    "#Read into the data file\n",
    "cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1,2),unpack=True) #Read in comma separated data\n",
    "#Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size # number of training examples\n",
    "#Insert the usual column of 1's into the \"X\" matrix\n",
    "X = np.insert(X,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quick visualize data\n",
    "plt.grid(True)\n",
    "plt.xlim([-100,5000])\n",
    "dummy = plt.hist(X[:,0],label = 'col1')\n",
    "dummy = plt.hist(X[:,1],label = 'col2')\n",
    "dummy = plt.hist(X[:,2],label = 'col3')\n",
    "plt.title('Clearly we need feature normalization.')\n",
    "plt.xlabel('Column Value')\n",
    "plt.ylabel('Counts')\n",
    "dummy = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature normalizing the columns (subtract mean, divide by standard deviation)\n",
    "#Store the mean and std for later use\n",
    "#Note don't modify the original X matrix, use a copy\n",
    "stored_feature_means, stored_feature_stds = [], []\n",
    "Xnorm = X.copy()\n",
    "for icol in xrange(Xnorm.shape[1]):\n",
    "    stored_feature_means.append(np.mean(Xnorm[:,icol]))\n",
    "    stored_feature_stds.append(np.std(Xnorm[:,icol]))\n",
    "    #Skip the first column\n",
    "    if not icol: continue\n",
    "    #Faster to not recompute the mean and std again, just used stored values\n",
    "    Xnorm[:,icol] = (Xnorm[:,icol] - stored_feature_means[-1])/stored_feature_stds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Quick visualize the feature-normalized data\n",
    "plt.grid(True)\n",
    "plt.xlim([-5,5])\n",
    "dummy = plt.hist(Xnorm[:,0],label = 'col1')\n",
    "dummy = plt.hist(Xnorm[:,1],label = 'col2')\n",
    "dummy = plt.hist(Xnorm[:,2],label = 'col3')\n",
    "plt.title('Feature Normalization Accomplished')\n",
    "plt.xlabel('Column Value')\n",
    "plt.ylabel('Counts')\n",
    "dummy = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run gradient descent with multiple variables, initial theta still set to zeros\n",
    "#(Note! This doesn't work unless we feature normalize! \"overflow encountered in multiply\")\n",
    "initial_theta = np.zeros((Xnorm.shape[1],1))\n",
    "theta, thetahistory, jvec = descendGradient(Xnorm,initial_theta)\n",
    "\n",
    "#Plot convergence of cost function:\n",
    "plotConvergence(jvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Final result theta parameters: \\n\",theta\n",
    "print \"\\nCheck of result: What is price of house with 1650 square feet and 3 bedrooms?\"\n",
    "ytest = np.array([1650.,3.])\n",
    "#To \"undo\" feature normalization, we \"undo\" 1650 and 3, then plug it into our hypothesis\n",
    "ytestscaled = [(ytest[x]-stored_feature_means[x+1])/stored_feature_stds[x+1] for x in xrange(len(ytest))]\n",
    "ytestscaled.insert(0,1)\n",
    "print ytestscaled\n",
    "#print \"$%0.2f\" % float(h(theta,ytestscaled))\n",
    "#print ytest\n",
    "#print ytest-stored_feature_means[1:]\n",
    "#print stored_feature_means[1:]\n",
    "ytestscaled = np.array((ytest-stored_feature_means[1:])/stored_feature_means[1:])\n",
    "#print ytestscaled\n",
    "ytestscaled = np.insert(np.array(ytestscaled),0,1)\n",
    "print ytestscaled\n",
    "#print \"$%0.2f\" % float(h(theta,ytestscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "#Implementation of normal equation to find analytic solution to linear regression\n",
    "def normEqtn(X,y):\n",
    "    #restheta = np.zeros((X.shape[1],1))\n",
    "    return np.dot(np.dot(inv(np.dot(X.T,X)),X.T),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print normEqtn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Normal equation prediction for price of house with 1650 square feet and 3 bedrooms\"\n",
    "print \"$%0.2f\" % float(h(normEqtn(X,y),[1,1650.,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
