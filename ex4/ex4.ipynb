{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 3: Neural Networks Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "from scipy.special import expit #Vectorized sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note this is actually a symlink... same data as last exercise,\n",
    "#so there's no reason to add another 7MB to my github repo...\n",
    "datafile = 'data/ex4data1.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "X, y = mat['X'], mat['y']\n",
    "#Insert a column of 1's to X as usual\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "print \"'y' shape: %s. Unique elements in y: %s\"%(mat['y'].shape,np.unique(mat['y']))\n",
    "print \"'X' shape: %s. X[0] shape: %s\"%(X.shape,X[0].shape)\n",
    "#X is 5000 images. Each image is a row. Each image has 400 pixels unrolled (20x20)\n",
    "#y is a classification for each image. 1-10, where \"10\" is the handwritten \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDatumImg(row):\n",
    "    \"\"\"\n",
    "    Function that is handed a single np array with shape 1x400,\n",
    "    crates an image object from it, and returns it\n",
    "    \"\"\"\n",
    "    width, height = 20, 20\n",
    "    square = row[1:].reshape(width,height)\n",
    "    return square.T\n",
    "    \n",
    "def displayData(indices_to_display = None):\n",
    "    \"\"\"\n",
    "    Function that picks 100 random rows from X, creates a 20x20 image from each,\n",
    "    then stitches them together into a 10x10 grid of images, and shows it.\n",
    "    \"\"\"\n",
    "    width, height = 20, 20\n",
    "    nrows, ncols = 10, 10\n",
    "    if not indices_to_display:\n",
    "        indices_to_display = random.sample(range(X.shape[0]), nrows*ncols)\n",
    "        \n",
    "    big_picture = np.zeros((height*nrows,width*ncols))\n",
    "    \n",
    "    irow, icol = 0, 0\n",
    "    for idx in indices_to_display:\n",
    "        if icol == ncols:\n",
    "            irow += 1\n",
    "            icol  = 0\n",
    "        iimg = getDatumImg(X[idx])\n",
    "        big_picture[irow*height:irow*height+iimg.shape[0],icol*width:icol*width+iimg.shape[1]] = iimg\n",
    "        icol += 1\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    img = scipy.misc.toimage( big_picture )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You have been provided with a set of network parameters (Θ(1),Θ(2)) \n",
    "#already trained by us. These are stored in ex4weights.mat\n",
    "datafile = 'data/ex4weights.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "Theta1, Theta2 = mat['Theta1'], mat['Theta2']\n",
    "# The matrices Theta1 and Theta2 will now be in your workspace\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "print \"Theta1 has shape:\",Theta1.shape\n",
    "print \"Theta2 has shape:\",Theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are some global variables I'm suing to ensure the sizes\n",
    "# of various matrices are correct\n",
    "#these are NOT including bias nits\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "output_layer_size = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#blah = np.ones((3,4))\n",
    "#np.insert(a, 1, 5, axis=1)\n",
    "#print np.insert(blah,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def flattenParams(thetas_list):\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def reshapeParams(flattened_array):\n",
    "    theta1 = flattened_array[:(input_layer_size+1)*hidden_layer_size] \\\n",
    "            .reshape((hidden_layer_size,input_layer_size+1))\n",
    "    theta2 = flattened_array[(input_layer_size+1)*hidden_layer_size:] \\\n",
    "            .reshape((output_layer_size,hidden_layer_size+1))\n",
    "    \n",
    "    return [ theta1, theta2 ]\n",
    "\n",
    "myThetas = [ Theta1, Theta2 ]\n",
    "print Theta1.shape\n",
    "print Theta2.shape\n",
    "flattened = flattenParams(myThetas)\n",
    "print \"flattend shape = \",flattened.shape\n",
    "reshaped = reshapeParams(flattened)\n",
    "print \"reshaped length = \",len(reshaped)\n",
    "print \"reshaped inner shapes: \",[inner.shape for inner in reshaped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Feedforward and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeCost(mythetas_flattened,myX,myy,mylambda=0.):\n",
    "    \"\"\"\n",
    "    This function takes in:\n",
    "        1) a flattened vector of theta parameters (each theta would go from one\n",
    "           NN layer to the next), the thetas do not include the bias unit.\n",
    "           The thetas are NOT \"unrolled\" in this function\n",
    "        2) the training set matrix X, which contains the bias unit first column\n",
    "        3) the label vector y, which has one column\n",
    "    It loops over training points (recommended by the professor, as the linear\n",
    "    algebra version is \"quite complicated\") and, for each, it:\n",
    "        1) loops over the each training point, and for each, it\n",
    "            2) constructs a new \"y\" vector, with 10 rows and 1 column, \n",
    "                with one non-zero entry corresponding to that iteration\n",
    "            3) computes the cost given that y- vector and that training point\n",
    "        4) sums up all of the costs\n",
    "        5) computes a regularization term\n",
    "    \"\"\"\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    #This is what will accumulate the total cost\n",
    "    total_cost = 0.\n",
    "    \n",
    "    m = myX.shape[0] #5000\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "                \n",
    "        # First compute the hypothesis (this is a (10,1) vector\n",
    "        # of the hypothesis for each possible y-value)\n",
    "        # propagateForward returns (zs, activations) for each layer\n",
    "        # so propagateforward[-1][1] means \"activation for -1st (last) layer\"\n",
    "        myhs = propagateForward(myrow,mythetas)[-1][1]\n",
    "\n",
    "        # Construct a 10x1 \"y\" vector with all zeros and only one \"1\" entry\n",
    "        # note here if the hand-written digit is \"0\", then that corresponds\n",
    "        # to a y- vector with 1 in the 10th spot (different from what the\n",
    "        # homework suggests)\n",
    "        tmpy  = np.zeros((10,1))\n",
    "        tmpy[myy[irow]-1] = 1\n",
    "        \n",
    "        # Compute the cost for this point and y-vector\n",
    "        mycost = -tmpy.T.dot(np.log(myhs))-(1-tmpy.T).dot(np.log(1-myhs))\n",
    "     \n",
    "        # Accumulate the total cost\n",
    "        total_cost += mycost\n",
    "  \n",
    "    # Normalize the total_cost, cast as float\n",
    "    total_cost = float(total_cost) / m\n",
    "    \n",
    "    # Compute the regularization term\n",
    "    total_reg = 0.\n",
    "    for mytheta in mythetas:\n",
    "        total_reg += np.sum(mytheta*mytheta) #element-wise multiplication\n",
    "    total_reg *= float(mylambda)/(2*m)\n",
    "        \n",
    "    return total_cost + total_reg\n",
    "       \n",
    "\n",
    "def propagateForward(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that given a list of Thetas, propagates the\n",
    "    Row of features forwards, assuming the features ALREADY\n",
    "    include the bias unit in the input layer, and the \n",
    "    Thetas need the bias unit added to features between each layer\n",
    "\n",
    "    The output is a vector with element [0] for the hidden layer,\n",
    "    and element [1] for the output layer\n",
    "        -- Each element is a tuple of (zs, as)\n",
    "        -- where \"zs\" and \"as\" have shape (# of units in that layer, 1)\n",
    "    \n",
    "    ***The 'activations' are the same as \"h\", but this works for many layers\n",
    "    (hence a vector of thetas, not just one theta)\n",
    "    Also, \"h\" is vectorized to do all rows at once...\n",
    "    this function takes in one row at a time***\n",
    "    \"\"\"\n",
    "    \n",
    "    #HEY DAVID REWRITE THIS SHITTY HANDLE ON WHETHER THETAS WAS FLATTENED OR NOT\n",
    "    #Theta1 has shape: (25, 401)\n",
    "    #Theta2 has shape: (10, 26)\n",
    "    #Thetas = np.array(Thetas)\n",
    "    #shapedTheta0 = np.zeros((25,400))\n",
    "    #shapedTheta1 = np.zeros((10,25))\n",
    "    #if Thetas.shape[0] == 10285:\n",
    "    #    np.insert(shapedTheta0,0,0,axis=1)\n",
    "    #    np.insert(shapedTheta1,0,0,axis=1)\n",
    "    #    shapedTheta0 = Thetas[:10025].reshape((25,401))\n",
    "    #    shapedTheta1 = Thetas[10025:].reshape((10,26))\n",
    "     \n",
    "        \n",
    "    #if Thetas.shape[0] == 10250:\n",
    "    #    shapedTheta0 = Thetas[:10000].reshape((25,400))\n",
    "    #    shapedTheta1 = Thetas[10000:].reshape((10,25))\n",
    "    #    #add bias units\n",
    "    #    np.insert(shapedTheta0,0,1,axis=1)\n",
    "    #    np.insert(shapedTheta1,0,1,axis=1)\n",
    "    #    Thetas = []\n",
    "    #    Thetas.append(shapedTheta0)\n",
    "    #    Thetas.append(shapedTheta1)\n",
    "   \n",
    "    features = row\n",
    "    zs_as_per_layer = []\n",
    "    for i in xrange(len(Thetas)):      \n",
    "        Theta = Thetas[i]\n",
    "        z = Theta.dot(features).reshape((Theta.shape[0],1))\n",
    "        a = expit(z)\n",
    "        zs_as_per_layer.append( (z, a) )\n",
    "        if i == len(Thetas)-1:\n",
    "            return np.array(zs_as_per_layer)\n",
    "        a = np.insert(a,0,1) #Add the bias unit\n",
    "        features = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myThetas = [ Theta1, Theta2 ]\n",
    "test = propagateForward(X[0],myThetas)\n",
    "print test[0]\n",
    "print test[1][1].shape\n",
    "#for zs, activations in test:\n",
    "#    print \"zs shape = \",zs.shape\n",
    "#    print \"as shape = \",activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Once you are done, using the loaded set of parameters Theta1 and Theta2,\n",
    "#you should see that the cost is about 0.287629\n",
    "myThetas = [ Theta1, Theta2 ]\n",
    "\n",
    "#Note I flatten the thetas vector before handing it to the computeCost routine,\n",
    "#as per the input format of the computeCost function.\n",
    "#It does the unrolling/reshaping itself\n",
    "print computeCost(flattenParams(myThetas),X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Regularized cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Once you are done, using the loaded set of parameters Theta1 and Theta2,\n",
    "#and lambda = 1, you should see that the cost is about 0.383770\n",
    "myThetas = [ Theta1, Theta2 ]\n",
    "print computeCost(flattenParams(myThetas),X,y,mylambda=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1 Sigmoid gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    dummy = expit(z)\n",
    "    return dummy*(1-dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genRandThetas():\n",
    "    epsilon_init = 0.12\n",
    "    theta1_shape = (hidden_layer_size, input_layer_size+1)\n",
    "    theta2_shape = (output_layer_size, hidden_layer_size+1)\n",
    "    rand_thetas = [ np.random.rand( *theta1_shape ) * 2 * epsilon_init - epsilon_init, \\\n",
    "                    np.random.rand( *theta2_shape ) * 2 * epsilon_init - epsilon_init]\n",
    "    return rand_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#myThetas = [ Theta1, Theta2 ]\n",
    "#myhs = propagateForward(X[0],myThetas)\n",
    "#print myhs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backPropagate(myX,myy,mythetas):\n",
    "    #input_layer_size = 400\n",
    "    #hidden_layer_size = 25\n",
    "    #output_layer_size = 10 \n",
    "    m = myX.shape[0] #5000\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Question: what shape should Delta have? Does it include hidden layers?\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Delta1 = np.zeros((hidden_layer_size,input_layer_size))\n",
    "    Delta2 = np.zeros((output_layer_size,hidden_layer_size))\n",
    "    m = 1\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "        a1 = myrow[1:].reshape((400,1))\n",
    "        z1 = expit(a1)#[1:].reshape((400,1))\n",
    "        #print \"z1 shape is \",z1.shape\n",
    "        # propagateForward returns (zs, activations) for each layer excluding the input layer\n",
    "        temp = propagateForward(myrow,mythetas)\n",
    "        z2 = temp[0][0]\n",
    "        #print \"z2 shape is \",z2.shape\n",
    "        a2 = temp[0][1]\n",
    "        z3 = temp[1][0]\n",
    "        #print \"z3 shape is \",z3.shape\n",
    "        a3 = temp[1][1]\n",
    "        tmpy  = np.zeros((10,1))\n",
    "        tmpy[myy[irow]-1] = 1\n",
    "        delta3 = a3 - tmpy \n",
    "        delta2 = mythetas[1].T.dot(delta3)[1:]*sigmoidGradient(z2) #remove 0th element\n",
    "        delta1 = mythetas[0].T.dot(delta2)[1:]*sigmoidGradient(z1) #remove 0th element\n",
    "        Delta1 += delta2.dot(a1.T)\n",
    "        Delta2 += delta3.dot(a2.T)\n",
    "        print \"delta1 shape is \",delta1.shape\n",
    "        print \"delta2 shape is \",delta2.shape\n",
    "        print \"Delta2 shape is \",delta3.dot(a2.T).shape\n",
    "        print \"Delta1 shape is \",delta2.dot(a1.T).shape\n",
    "    D1 = Delta1/float(m)\n",
    "    D2 = Delta2/float(m)\n",
    "    return D1, D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D1, D2 = backPropagate(X,y,myThetas)\n",
    "print D1.shape\n",
    "print D2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkGradient(mythetas,myX,myy,mylambda=0.):\n",
    "    myeps = 0.0001\n",
    "    myshapes = [ myTheta[:,1:].shape for myTheta in mythetas ]\n",
    "    print myshapes\n",
    "    flattened = list(mythetas[0][:,1:].flatten()) + list(mythetas[1][:,1:].flatten())\n",
    "    flattened = np.array(flattened).reshape((len(flattened),1))\n",
    "    print flattened.shape\n",
    "    n_elems = len(flattened)\n",
    "    print n_elems\n",
    "    grad = np.zeros((n_elems,1))\n",
    "    for x in xrange(1):#n_elems):\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        epsvec[x] = myeps\n",
    "        #print \"flattened shape is \",flattened.shape\n",
    "        cost_high = computeCost(flattened + epsvec,myX,myy,mylambda)\n",
    "        cost_low  = computeCost(flattened - epsvec,myX,myy,mylambda)\n",
    "        mygrad = (cost_high - cost_low) / float(2*myeps)\n",
    "        print mygrad\n",
    "        grad[x] = mygrad\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is all c/p old stuff from the previous week(s) and should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def computeCostDELETEME(mytheta,myX,myy,mylambda = 0.):\n",
    "    m = myX.shape[0] #5000\n",
    "    myh = h(mytheta,myX) #shape: (5000,1)\n",
    "    term1 = np.log( myh ).dot( -myy.T ) #shape: (5000,5000)\n",
    "    term2 = np.log( 1.0 - myh ).dot( 1 - myy.T ) #shape: (5000,5000)\n",
    "    left_hand = (term1 - term2) / m #shape: (5000,5000)\n",
    "    right_hand = mytheta.T.dot( mytheta ) * mylambda / (2*m) #shape: (1,1)\n",
    "    return left_hand + right_hand #shape: (5000,5000)\n",
    "\n",
    "def computeCost(mythetas,myX,myy)\n",
    "    \"\"\"\n",
    "    This function takes in:\n",
    "        1) a vector of thetas (each theta goes from one\n",
    "           NN layer to the next), the thetas do not include the bias unit.\n",
    "        2) the training set matrix X, which contains the bias unit first column\n",
    "        3) the label vector y, which has one column\n",
    "    It loops over training points (recommended by the professor, as the linear\n",
    "    algebra version is \"quite complicated\") and, for each, it:\n",
    "        1) loops over the each training point, and for each, it\n",
    "            2) constructs a new \"y\" vector, with 10 rows and 1 column, \n",
    "                with one non-zero entry corresponding to that iteration\n",
    "            3) computes the cost given that y- vector and that training point\n",
    "        4) sums up all of the costs\n",
    "        \n",
    "            2b) computes the cost for each of 10 possible outcomes\n",
    "    \"\"\"\n",
    "    m = myX.shape[0] #5000\n",
    "    for training_pt in myX\n",
    "\n",
    "def propagateForward(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that given a list of Thetas, propagates the\n",
    "    Row of features forwards, assuming the features already\n",
    "    include the bias unit in the input layer, and the \n",
    "    Thetas need the bias unit added to features between each layer\n",
    "    \n",
    "    ***This is the same as \"h\", but this works for many layers\n",
    "    (hence a vector of thetas, not just one theta)\n",
    "    Also, \"h\" is vectorized to do all rows at once...\n",
    "    this function takes in one row at a time***\n",
    "    \n",
    "    \"\"\"\n",
    "    features = row\n",
    "    for i in xrange(len(Thetas)):\n",
    "        Theta = Thetas[i]\n",
    "        z = Theta.dot(features)\n",
    "        a = expit(z)\n",
    "        if i == len(Thetas)-1:\n",
    "            return a\n",
    "        a = np.insert(a,0,1) #Add the bias unit\n",
    "        features = a\n",
    "\n",
    "def predictNN(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that takes a row of features, propagates them through the\n",
    "    NN, and returns the predicted integer that was hand written\n",
    "    \"\"\"\n",
    "    classes = range(1,10) + [10]\n",
    "    output = propagateForward(row,Thetas)\n",
    "    return classes[np.argmax(np.array(output))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myThetas = [ Theta1, Theta2 ]\n",
    "#Loop over all of the rows in X (all of the handwritten images)\n",
    "#and predict what digit is written. Check if it's correct, and\n",
    "#compute an efficiency.\n",
    "for irow in xrange(X.shape[0]):\n",
    "    if predictNN(X[irow],myThetas) == int(y[irow]): \n",
    "        n_correct += 1\n",
    "    else: incorrect_indices.append(irow)\n",
    "print \"Training set accuracy: %0.1f%%\"%(100*(n_correct/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Once you are done, ex4.m will call your nnCostFunction \n",
    "# using the loaded set of parameters for Theta1 and Theta2. \n",
    "# You should see that the cost is about 0.287629."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Regularized cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hypothesis function and cost function for logistic regression\n",
    "def h(mytheta,myX): #Logistic hypothesis function\n",
    "    return expit(np.dot(myX,mytheta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#An alternative to OCTAVE's 'fmincg' we'll use some scipy.optimize function, \"fmin_cg\"\n",
    "#This is more efficient with large number of parameters.\n",
    "#In the previous homework, I didn't have to compute the cost gradient because\n",
    "#the scipy.optimize function did it for me with some kind of interpolation...\n",
    "#However, fmin_cg needs the gradient handed do it, so I'll implement that here\n",
    "def costGradient(mytheta,myX,myy,mylambda = 0.):\n",
    "    m = myX.shape[0]\n",
    "    #Tranpose y here because it makes the units work out in dot products later\n",
    "    #(with the way I've written them, anyway)\n",
    "    beta = h(mytheta,myX)-myy.T #shape: (5000,5000)\n",
    "\n",
    "    #regularization skips the first element in theta\n",
    "    regterm = mytheta[1:]*(mylambda/m) #shape: (400,1)\n",
    "\n",
    "    grad = (1./m)*np.dot(myX.T,beta) #shape: (401, 5000)\n",
    "    #regularization skips the first element in theta\n",
    "    grad[1:] = grad[1:] + regterm\n",
    "    return grad #shape: (401, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def optimizeTheta(mytheta,myX,myy,mylambda=0.):\n",
    "    result = optimize.fmin_cg(computeCost, fprime=costGradient, x0=mytheta, \\\n",
    "                              args=(myX, myy, mylambda), maxiter=50, disp=False,\\\n",
    "                              full_output=True)\n",
    "    return result[0], result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note: I spent a LONG time trying to optimize everything. Initially training 10 classes\n",
    "#took about 5 minutes. Now I've got it down to taking ~5 seconds total!\n",
    "def buildTheta():\n",
    "    \"\"\"\n",
    "    Function that determines an optimized theta for each class\n",
    "    and returns a Theta function where each row corresponds\n",
    "    to the learned logistic regression params for one class\n",
    "    \"\"\"\n",
    "    mylambda = 0.\n",
    "    initial_theta = np.zeros((X.shape[1],1)).reshape(-1)\n",
    "    Theta = np.zeros((10,X.shape[1]))\n",
    "    for i in xrange(10):\n",
    "        iclass = i if i else 10 #class \"10\" corresponds to handwritten zero\n",
    "        print \"Optimizing for handwritten number %d...\"%i\n",
    "        logic_Y = np.array([1 if x == iclass else 0 for x in y])#.reshape((X.shape[0],1))\n",
    "        itheta, imincost = optimizeTheta(initial_theta,X,logic_Y,mylambda)\n",
    "        Theta[i,:] = itheta\n",
    "    print \"Done!\"\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Theta = buildTheta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictOneVsAll(myTheta,myrow):\n",
    "    \"\"\"\n",
    "    Function that computes a hypothesis for an individual image (row in X)\n",
    "    and returns the predicted integer corresponding to the handwritten image\n",
    "    \"\"\"\n",
    "    classes = [10] + range(1,10)\n",
    "    hypots  = [0]*len(classes)\n",
    "    #Compute a hypothesis for each possible outcome\n",
    "    #Choose the maximum hypothesis to find result\n",
    "    for i in xrange(len(classes)):\n",
    "        hypots[i] = h(myTheta[i],myrow)\n",
    "    return classes[np.argmax(np.array(hypots))]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"You should see that the training set accuracy is about 94.9%\"\n",
    "n_correct, n_total = 0., 0.\n",
    "incorrect_indices = []\n",
    "for irow in xrange(X.shape[0]):\n",
    "    n_total += 1\n",
    "    if predictOneVsAll(Theta,X[irow]) == y[irow]: \n",
    "        n_correct += 1\n",
    "    else: incorrect_indices.append(irow)\n",
    "print \"Training set accuracy: %0.1f%%\"%(100*(n_correct/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's have a look at the ones we get wrong:\n",
    "displayData(incorrect_indices[:100])\n",
    "displayData(incorrect_indices[100:200])\n",
    "displayData(incorrect_indices[200:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Model representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Feedforward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def propagateForward(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that given a list of Thetas, propagates the\n",
    "    Row of features forwards, assuming the features already\n",
    "    include the bias unit in the input layer, and the \n",
    "    Thetas need the bias unit added to features between each layer\n",
    "    \"\"\"\n",
    "    features = row\n",
    "    for i in xrange(len(Thetas)):\n",
    "        Theta = Thetas[i]\n",
    "        z = Theta.dot(features)\n",
    "        a = expit(z)\n",
    "        if i == len(Thetas)-1:\n",
    "            return a\n",
    "        a = np.insert(a,0,1) #Add the bias unit\n",
    "        features = a\n",
    "\n",
    "def predictNN(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that takes a row of features, propagates them through the\n",
    "    NN, and returns the predicted integer that was hand written\n",
    "    \"\"\"\n",
    "    classes = range(1,10) + [10]\n",
    "    output = propagateForward(row,Thetas)\n",
    "    return classes[np.argmax(np.array(output))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"You should see that the accuracy is about 97.5%\"\n",
    "myThetas = [ Theta1, Theta2 ]\n",
    "n_correct, n_total = 0., 0.\n",
    "incorrect_indices = []\n",
    "#Loop over all of the rows in X (all of the handwritten images)\n",
    "#and predict what digit is written. Check if it's correct, and\n",
    "#compute an efficiency.\n",
    "for irow in xrange(X.shape[0]):\n",
    "    n_total += 1\n",
    "    if predictNN(X[irow],myThetas) == int(y[irow]): \n",
    "        n_correct += 1\n",
    "    else: incorrect_indices.append(irow)\n",
    "print \"Training set accuracy: %0.1f%%\"%(100*(n_correct/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pick some of the images we got WRONG and look at them, just to see\n",
    "for x in xrange(5):\n",
    "    i = random.choice(incorrect_indices)\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    img = scipy.misc.toimage( getDatumImg(X[i]) )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "    predicted_val = predictNN(X[i],myThetas)\n",
    "    predicted_val = 0 if predicted_val == 10 else predicted_val\n",
    "    fig.suptitle('Predicted: %d'%predicted_val, fontsize=14, fontweight='bold')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
