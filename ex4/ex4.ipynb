{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 3: Neural Networks Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "import scipy.optimize #fmin_cg to train neural network\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note this is actually a symlink... same data as last exercise,\n",
    "#so there's no reason to add another 7MB to my github repo...\n",
    "datafile = 'data/ex4data1.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "X, y = mat['X'], mat['y']\n",
    "#Insert a column of 1's to X as usual\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "print \"'y' shape: %s. Unique elements in y: %s\"%(mat['y'].shape,np.unique(mat['y']))\n",
    "print \"'X' shape: %s. X[0] shape: %s\"%(X.shape,X[0].shape)\n",
    "#X is 5000 images. Each image is a row. Each image has 400 pixels unrolled (20x20)\n",
    "#y is a classification for each image. 1-10, where \"10\" is the handwritten \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDatumImg(row):\n",
    "    \"\"\"\n",
    "    Function that is handed a single np array with shape 1x400,\n",
    "    crates an image object from it, and returns it\n",
    "    \"\"\"\n",
    "    width, height = 20, 20\n",
    "    square = row[1:].reshape(width,height)\n",
    "    return square.T\n",
    "    \n",
    "def displayData(indices_to_display = None):\n",
    "    \"\"\"\n",
    "    Function that picks 100 random rows from X, creates a 20x20 image from each,\n",
    "    then stitches them together into a 10x10 grid of images, and shows it.\n",
    "    \"\"\"\n",
    "    width, height = 20, 20\n",
    "    nrows, ncols = 10, 10\n",
    "    if not indices_to_display:\n",
    "        indices_to_display = random.sample(range(X.shape[0]), nrows*ncols)\n",
    "        \n",
    "    big_picture = np.zeros((height*nrows,width*ncols))\n",
    "    \n",
    "    irow, icol = 0, 0\n",
    "    for idx in indices_to_display:\n",
    "        if icol == ncols:\n",
    "            irow += 1\n",
    "            icol  = 0\n",
    "        iimg = getDatumImg(X[idx])\n",
    "        big_picture[irow*height:irow*height+iimg.shape[0],icol*width:icol*width+iimg.shape[1]] = iimg\n",
    "        icol += 1\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    img = scipy.misc.toimage( big_picture )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You have been provided with a set of network parameters (Θ(1),Θ(2)) \n",
    "#already trained by us. These are stored in ex4weights.mat\n",
    "datafile = 'data/ex4weights.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "Theta1, Theta2 = mat['Theta1'], mat['Theta2']\n",
    "# The matrices Theta1 and Theta2 will now be in your workspace\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are some global variables I'm suing to ensure the sizes\n",
    "# of various matrices are correct\n",
    "#these are NOT including bias nits\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "output_layer_size = 10 \n",
    "n_training_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Some utility functions. There are lot of flattening and\n",
    "#reshaping of theta matrices, the input X matrix, etc...\n",
    "#Nicely shaped matrices make the linear algebra easier when developing,\n",
    "#but the minimization routine (fmin_cg) requires that all inputs\n",
    "\n",
    "def flattenParams(thetas_list):\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def reshapeParams(flattened_array):\n",
    "    theta1 = flattened_array[:(input_layer_size+1)*hidden_layer_size] \\\n",
    "            .reshape((hidden_layer_size,input_layer_size+1))\n",
    "    theta2 = flattened_array[(input_layer_size+1)*hidden_layer_size:] \\\n",
    "            .reshape((output_layer_size,hidden_layer_size+1))\n",
    "    \n",
    "    return [ theta1, theta2 ]\n",
    "\n",
    "def flattenX(myX):\n",
    "    return np.array(myX.flatten()).reshape((n_training_samples*(input_layer_size+1),1))\n",
    "\n",
    "def reshapeX(flattenedX):\n",
    "    return np.array(flattenedX).reshape((n_training_samples,input_layer_size+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Feedforward and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeCost(mythetas_flattened,myX_flattened,myy,mylambda=0.):\n",
    "    \"\"\"\n",
    "    This function takes in:\n",
    "        1) a flattened vector of theta parameters (each theta would go from one\n",
    "           NN layer to the next), the thetas include the bias unit.\n",
    "        2) the flattened training set matrix X, which contains the bias unit first column\n",
    "        3) the label vector y, which has one column\n",
    "    It loops over training points (recommended by the professor, as the linear\n",
    "    algebra version is \"quite complicated\") and:\n",
    "        1) constructs a new \"y\" vector, with 10 rows and 1 column, \n",
    "            with one non-zero entry corresponding to that iteration\n",
    "        2) computes the cost given that y- vector and that training point\n",
    "        3) accumulates all of the costs\n",
    "        4) computes a regularization term (after the loop over training points)\n",
    "    \"\"\"\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    # Now unroll X\n",
    "    myX = reshapeX(myX_flattened)\n",
    "    \n",
    "    #This is what will accumulate the total cost\n",
    "    total_cost = 0.\n",
    "    \n",
    "    m = n_training_samples\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "                \n",
    "        # First compute the hypothesis (this is a (10,1) vector\n",
    "        # of the hypothesis for each possible y-value)\n",
    "        # propagateForward returns (zs, activations) for each layer\n",
    "        # so propagateforward[-1][1] means \"activation for -1st (last) layer\"\n",
    "        myhs = propagateForward(myrow,mythetas)[-1][1]\n",
    "\n",
    "        # Construct a 10x1 \"y\" vector with all zeros and only one \"1\" entry\n",
    "        # note here if the hand-written digit is \"0\", then that corresponds\n",
    "        # to a y- vector with 1 in the 10th spot (different from what the\n",
    "        # homework suggests)\n",
    "        tmpy  = np.zeros((10,1))\n",
    "        tmpy[myy[irow]-1] = 1\n",
    "        \n",
    "        # Compute the cost for this point and y-vector\n",
    "        mycost = -tmpy.T.dot(np.log(myhs))-(1-tmpy.T).dot(np.log(1-myhs))\n",
    "     \n",
    "        # Accumulate the total cost\n",
    "        total_cost += mycost\n",
    "  \n",
    "    # Normalize the total_cost, cast as float\n",
    "    total_cost = float(total_cost) / m\n",
    "    \n",
    "    # Compute the regularization term\n",
    "    total_reg = 0.\n",
    "    for mytheta in mythetas:\n",
    "        total_reg += np.sum(mytheta*mytheta) #element-wise multiplication\n",
    "    total_reg *= float(mylambda)/(2*m)\n",
    "        \n",
    "    return total_cost + total_reg\n",
    "       \n",
    "\n",
    "def propagateForward(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that given a list of Thetas (NOT flattened), propagates the\n",
    "    row of features forwards, assuming the features ALREADY\n",
    "    include the bias unit in the input layer, and the \n",
    "    Thetas also include the bias unit\n",
    "\n",
    "    The output is a vector with element [0] for the hidden layer,\n",
    "    and element [1] for the output layer\n",
    "        -- Each element is a tuple of (zs, as)\n",
    "        -- where \"zs\" and \"as\" have shape (# of units in that layer, 1)\n",
    "    \n",
    "    ***The 'activations' are the same as \"h\", but this works for many layers\n",
    "    (hence a vector of thetas, not just one theta)\n",
    "    Also, \"h\" is vectorized to do all rows at once...\n",
    "    this function takes in one row at a time***\n",
    "    \"\"\"\n",
    "   \n",
    "    features = row\n",
    "    zs_as_per_layer = []\n",
    "    for i in xrange(len(Thetas)):  \n",
    "        Theta = Thetas[i]\n",
    "        #Theta is (25,401), features are (401, 1)\n",
    "        #so \"z\" comes out to be (25, 1)\n",
    "        #this is one \"z\" value for each unit in the hidden layer\n",
    "        #not counting the bias unit\n",
    "        z = Theta.dot(features).reshape((Theta.shape[0],1))\n",
    "        a = expit(z)\n",
    "        zs_as_per_layer.append( (z, a) )\n",
    "        if i == len(Thetas)-1:\n",
    "            return np.array(zs_as_per_layer)\n",
    "        a = np.insert(a,0,1) #Add the bias unit\n",
    "        features = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Once you are done, using the loaded set of parameters Theta1 and Theta2,\n",
    "#you should see that the cost is about 0.287629\n",
    "myThetas = [ Theta1, Theta2 ]\n",
    "\n",
    "#Note I flatten the thetas vector before handing it to the computeCost routine,\n",
    "#as per the input format of the computeCost function.\n",
    "#It does the unrolling/reshaping itself\n",
    "#I also flatten the X vector, similarly\n",
    "print computeCost(flattenParams(myThetas),flattenX(X),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Regularized cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Once you are done, using the loaded set of parameters Theta1 and Theta2,\n",
    "#and lambda = 1, you should see that the cost is about 0.383770\n",
    "myThetas = [ Theta1, Theta2 ]\n",
    "print computeCost(flattenParams(myThetas),flattenX(X),y,mylambda=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1 Sigmoid gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    dummy = expit(z)\n",
    "    return dummy*(1-dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genRandThetas():\n",
    "    epsilon_init = 0.12\n",
    "    theta1_shape = (hidden_layer_size, input_layer_size+1)\n",
    "    theta2_shape = (output_layer_size, hidden_layer_size+1)\n",
    "    rand_thetas = [ np.random.rand( *theta1_shape ) * 2 * epsilon_init - epsilon_init, \\\n",
    "                    np.random.rand( *theta2_shape ) * 2 * epsilon_init - epsilon_init]\n",
    "    return rand_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backPropagate(mythetas_flattened,myX_flattened,myy,mylambda=0.):\n",
    "    \n",
    "    # First unroll the parameters\n",
    "    mythetas = reshapeParams(mythetas_flattened)\n",
    "    \n",
    "    # Now unroll X\n",
    "    myX = reshapeX(myX_flattened)\n",
    "\n",
    "    #Note: the Delta matrices should include the bias unit\n",
    "    #The Delta matrices have the same shape as the theta matrices\n",
    "    Delta1 = np.zeros((hidden_layer_size,input_layer_size+1))\n",
    "    Delta2 = np.zeros((output_layer_size,hidden_layer_size+1))\n",
    "\n",
    "    # Loop over the training points (rows in myX, already contain bias unit)\n",
    "    m = n_training_samples\n",
    "    for irow in xrange(m):\n",
    "        myrow = myX[irow]\n",
    "        a1 = myrow.reshape((input_layer_size+1,1))\n",
    "        # propagateForward returns (zs, activations) for each layer excluding the input layer\n",
    "        temp = propagateForward(myrow,mythetas)\n",
    "        z2 = temp[0][0]\n",
    "        a2 = temp[0][1]\n",
    "        z3 = temp[1][0]\n",
    "        a3 = temp[1][1]\n",
    "        tmpy = np.zeros((10,1))\n",
    "        tmpy[myy[irow]-1] = 1\n",
    "        delta3 = a3 - tmpy \n",
    "        delta2 = mythetas[1].T[1:,:].dot(delta3)*sigmoidGradient(z2) #remove 0th element\n",
    "        a2 = np.insert(a2,0,1,axis=0)\n",
    "        Delta1 += delta2.dot(a1.T) #(25,1)x(1,401) = (25,401) (correct)\n",
    "        Delta2 += delta3.dot(a2.T) #(10,1)x(1,25) = (10,25) (should be 10,26)\n",
    "        \n",
    "    D1 = Delta1/float(m)\n",
    "    D2 = Delta2/float(m)\n",
    "    \n",
    "    #Regularization:\n",
    "    D1[:,1:] = D1[:,1:] + (float(mylambda)/m)*mythetas[0][:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (float(mylambda)/m)*mythetas[1][:,1:]\n",
    "    \n",
    "    return flattenParams([D1, D2]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Actually compute D matrices for the Thetas provided\n",
    "flattenedD1D2 = backPropagate(flattenParams(myThetas),flattenX(X),y,mylambda=0.)\n",
    "D1, D2 = reshapeParams(flattenedD1D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkGradient(mythetas,myDs,myX,myy,mylambda=0.):\n",
    "    myeps = 0.0001\n",
    "    flattened = flattenParams(mythetas)\n",
    "    flattenedDs = flattenParams(myDs)\n",
    "    myX_flattened = flattenX(myX)\n",
    "    n_elems = len(flattened) \n",
    "    #Pick ten random elements, compute numerical gradient, compare to respective D's\n",
    "    for i in xrange(10):\n",
    "        x = int(np.random.rand()*n_elems)\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        epsvec[x] = myeps\n",
    "        cost_high = computeCost(flattened + epsvec,myX_flattened,myy,mylambda)\n",
    "        cost_low  = computeCost(flattened - epsvec,myX_flattened,myy,mylambda)\n",
    "        mygrad = (cost_high - cost_low) / float(2*myeps)\n",
    "        print \"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"%(x,mygrad,flattenedDs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkGradient(myThetas,[D1, D2],X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Regularized Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#My back propagation already has regularization in it.\n",
    "#For now, I will assume the regularization part is correct\n",
    "#(since in this case the regularization code is simple, I'm quite confident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.5 Learning parameters using fmincg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here I will use scipy.optimize.fmin_cg\n",
    "\n",
    "def trainNN(mylambda=0.):\n",
    "    \"\"\"\n",
    "    Function that generates random initial theta matrices, optimizes them,\n",
    "    and returns a list of two re-shaped theta matrices\n",
    "    \"\"\"\n",
    "\n",
    "    randomThetas_unrolled = flattenParams(genRandThetas())\n",
    "    result = scipy.optimize.fmin_cg(computeCost, x0=randomThetas_unrolled, fprime=backPropagate, \\\n",
    "                               args=(flattenX(X),y,mylambda),maxiter=50,disp=True,full_output=True)\n",
    "    return reshapeParams(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training the NN takes about ~70-80 seconds on my machine\n",
    "learned_Thetas = trainNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If your implementation is correct, you should see a reported training accuracy of about 95.3%\n",
    "#(this may vary by about 1% due to the random initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictNN(row,Thetas):\n",
    "    \"\"\"\n",
    "    Function that takes a row of features, propagates them through the\n",
    "    NN, and returns the predicted integer that was hand written\n",
    "    \"\"\"\n",
    "    classes = range(1,10) + [10]\n",
    "    output = propagateForward(row,Thetas)\n",
    "    #-1 means last layer, 1 means \"a\" instead of \"z\"\n",
    "    return classes[np.argmax(output[-1][1])] \n",
    "\n",
    "def computeAccuracy(myX,myThetas,myy):\n",
    "    \"\"\"\n",
    "    Function that loops over all of the rows in X (all of the handwritten images)\n",
    "    and predicts what digit is written given the thetas. Check if it's correct, and\n",
    "    compute an efficiency.\n",
    "    \"\"\"\n",
    "    n_correct, n_total = 0, myX.shape[0]\n",
    "    for irow in xrange(n_total):\n",
    "        if int(predictNN(myX[irow],myThetas)) == int(myy[irow]): \n",
    "            n_correct += 1\n",
    "    print \"Training set accuracy: %0.1f%%\"%(100*(float(n_correct)/n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computeAccuracy(X,learned_Thetas,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see if I set lambda to 10, if I get the same thing\n",
    "learned_regularized_Thetas = trainNN(mylambda=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computeAccuracy(X,learned_regularized_Thetas,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Visualizing the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displayHiddenLayer(myTheta):\n",
    "    \"\"\"\n",
    "    Function that takes slices of the first Theta matrix (that goes from\n",
    "    the input layer to the hidden layer), removes the bias unit, and reshapes\n",
    "    it into a 20x20 image, and shows it\n",
    "    \"\"\"\n",
    "    #remove bias unit:\n",
    "    myTheta = myTheta[:,1:]\n",
    "    assert myTheta.shape == (25,400)\n",
    "    \n",
    "    width, height = 20, 20\n",
    "    nrows, ncols = 5, 5\n",
    "        \n",
    "    big_picture = np.zeros((height*nrows,width*ncols))\n",
    "    \n",
    "    irow, icol = 0, 0\n",
    "    for row in myTheta:\n",
    "        if icol == ncols:\n",
    "            irow += 1\n",
    "            icol  = 0\n",
    "        #add bias unit back in?\n",
    "        iimg = getDatumImg(np.insert(row,0,1))\n",
    "        big_picture[irow*height:irow*height+iimg.shape[0],icol*width:icol*width+iimg.shape[1]] = iimg\n",
    "        icol += 1\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    img = scipy.misc.toimage( big_picture )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayHiddenLayer(learned_Thetas[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
